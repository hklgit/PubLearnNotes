---
layout: post
author: 陶加涛
title: HyperLogLog非精确去重算法原理简析
date: 2019-06-26 09:21:01
tags:
  - BigData

categories: BigData
permalink: the-brief-principle-of-approximation-algorithm-hyperLogLog
---

### 1. 近似算法:HyperLogLog

利用 Roaring Bitmap 来进行精确去重，虽然这种算法能大大地减少存储开销，但是随着数据量的增大，它依然面临着存储上的压力。在这篇文章中将要介绍的 HyperLogLog（下称 HLL）是一种非精确的去重算法，它的特点是具有非常优异的空间复杂度（几乎可以达到常数级别）。

HLL 算法需要完整遍历所有元素一次，而非多次或采样；该算法只能计算集合中有多少个不重复的元素，不能给出每个元素的出现次数或是判断一个元素是否之前出现过；多个使用 HLL 统计出的基数值可以融合。

在牺牲一定准确性的前提下，存储空间可以被大量节省：
![]()

HLL 后面不同的数字代表着不同的精度，数字越大，精度越高，占用的空间也越大，可以认为 HLL 的空间占用只和精度成正相关。

HyperLogLog 的存储空间几乎不随基数的增加而增加:
![]()

HLL 算法有着非常优异的空间复杂度，可以看到它的空间占用随着基数值的增长并没有变化。

### 2. HLL算法原理感性认知

HLL 算法的原理会涉及到比较多的数学知识，这边对这些数学原理和证明不会展开。举一个生活中的例子来帮助大家理解HLL算法的原理：比如你在进行一个实验，内容是不停地抛硬币，记录你连续抛到正面的次数（这是数学中的伯努利过程，感兴趣同学可以自行研究下）；如果你最多的连抛正面记录是3次，那可以想象你并没有做这个实验太多次，如果你最长的连抛正面记录是 20 次，那你可能进行了这个实验上千次。

一种理论上存在的情况是，你非常幸运，第一次进行这个实验就连抛了 20 次正面，我们也会认为你进行了很多次这个实验才得到了这个记录，这就会导致错误的预估；改进的方式是请 10 位同学进行这项实验，这样就可以观察到更多的样本数据，降低出现上述情况的概率。这就是 HLL 算法的核心思想。










原文:[大数据分析常用去重算法分析HyperLogLog](https://mp.weixin.qq.com/s/SGVryEXETk-2OuUZdM5aOw)
