正确的为 Presto 集群配置内存并不是一件容易的事情。有许多不确定的因素会影响内存需求：
- Work 的个数
- Coordinator 与 Worker 的内存
- 数据源的类型与个数
- 用户数

对于 Presto 而言，资源管理是一个极具挑战性的问题。我们必须监视系统并适应当前以及即将到来的需求。让我们看一些细节，并讨论有关 Presto 中的内存管理和监视的建议和准则。

前述所有因素共同构成了我们所说的工作量。调整群集的内存在很大程度上取决于正在运行的工作负载。

例如，大多数查询形状都包含多个联接，聚合和窗口函数。 如果工作负载的查询大小较小，则可以为每个查询设置较低的内存限制并提高并发性，反之则可以设置较大的查询大小。 对于上下文，查询大小是查询形状和输入数据量的乘积。 Presto提供了一种通过在config.properties部署时设置某些属性来管理整个集群的内存利用率的方法：
- query.max-memory-per-node
- query.max-total-memory-per-node
- query.max-memory
- query.max-total-memory

Presto 中的内存管理分为两种内存分配：
- User Memory：用户查询（例如聚合和排序）控制着用户的内存分配。
- System Memory：系统内存分配基于查询引擎本身的执行实现，并且包括对缓冲区，表扫描和其他操作的读取，写入和 Shuffle。

考虑到这种分离，我们可以进一步检查内存属性：
- query.max-memory-per-node：单个 Query 在集群单个 Worker 上可以允许使用的最大 User Memory，主要用于处理聚合，输入数据分配等。
- query.max-total-memory-per-node：单个 Query 在集群单个 Worker 上可以允许使用的最大内存（ User Memory + System Memory），要求必须大于 query.max-memory-per-node。当 Query 消耗的内存超过此限制时，会被集群杀死。
- query.max-memory：单个 Query 在整个集群所有 Worker 上可以使用的最大 User Memory。
- query.max-total-memory：单个 Query 在整个集群所有 Worker 上可以使用的最大内存（User Memory + System Memory），必然也要大于 query.max-memory。

如果 Query 超过了这些限制并被杀死，会提示如下错误代码：
- EXCEEDED_LOCAL_MEMORY_LIMIT：表示超过 query.max-memory-per-node或 query.max-total-memory-per-node 的阈值。
- EXCEEDED_GLOBAL_MEMORY_LIMIT：表示超过 query.max-memory 或 query.max- total-memory 的阈值。

让我们看一下由一个 Coordinator 和数十个 Worker 组成的小型集群的实际示例以及特征：
- 一个 Coordinator；
- 十个 Worker。通常，Workers 都有相同的系统规格；
- 每个 Worker 的物理内存: 50 GB；
- `jvm.config` 配置文件中 -Xmx 配置的最大 JVM 堆大小：38 GB；
- query.max-memory-per-node: 13 GB；
- query.max-total-memory-per-node: 16 GB；
- query.max-memory: 50 GB；
- query.max-total-memory: 60 GB；
- memory.heap-headroom-per-node: 9 GB；

让我们进一步分解这些数字。每个 Worker 的总共可用内存为 50 GB。我们为操作系统、代理/守护进程以及运行在系统 JVM 之外的组件分配 12 GB，这些系统包括监控系统和其他系统，可以让我们管理机器并保持其正常运行。总之，我们决定将 JVM 堆大小设置为 38 GB。

当 Query 大小和形状较小时，可以将并发设置更大一点。在前面的示例中，我们假设 Query 大小和形状为中等到较大，并且还考虑了数据偏斜的情况。在整个集群级别 query.max-memory 设置为50 GB。在查看最大内存时，我们还考虑了 initial-hash-partitions 。理想情况下，该数字应小于或等于 Worker 数。

如果我们将其设置为8，最大内存为 50 GB，则得到 50/8，因此每个 Worker 大约为 6.25 GB。本地限制 max-memory-per-node 设置为 13 GB，我们通过允许每个节点使用两倍的内存来为数据歪斜保留一些余量。数据组织方式、运行的查询类型的不同会导致这些参数有很大差异。除此之外，集群的基础架构（例如，可用机器的大小以及数量）对理想配置也会有很大的影响。

可以通过一个配置来避免出现死锁情况：query.low-memory-killer.policy。可以设置为 total-reservation 或 total-reservation-on- blocked-nodes。当设置为 total-reservation 时，Presto 会杀死集群上运行最大的查询以释放资源。另一方面，total-reservation-on-blocked-nodes 会杀死被阻塞节点上使用最多内存的查询。

从示例中我们可以看到，我们需要做一些起始配置，然后根据实际负载进行动态调整。例如，使用可视化工具进行交互式查询可能会创建许多小型查询。然后，随着用户数的增加会导致查询数量以及并发度的增加。这通常不需要更改内存配置，而只需增加集群中的 Worker 数量即可。另一方面，如果同一集群添加了新的数据源，该数据源上有处理大数据量的复杂查询，并有可能超出限制，则必须调整内存。

需要我们注意的是按照最佳实践建议，在一个典型的 Presto 集群基础架构中，所有 Worker 都是相同的。全部使用相同硬件规格的虚拟机（VM）映像以及有相同的容器大小。因此，更改这些 Worker 的内存通常意味着新值对于物理可用内存而言太大，或者对于充分利用整个系统而言太小。因此，调整内存需要替换集群中的所有 Worker 节点。

还有最后一点需要注意，我们对工作负载的评估可以发现有很大的不同：许多查询是小型，快速，即席查询，几乎不占用内存，而有一些其他查询则是比较大，长期运行，其中包含大量分析查询，甚至使用非常不同的数据源。这些工作负载差异表明内存配置差异很大，甚至 Worker 配置差异很大，甚至有必要进行监视。在这种情况下，我们应该执行下一步，使用不同的 Presto 集群来分离工作负载。





- [Presto内存管理与相关参数设置](https://zhuanlan.zhihu.com/p/89381163)
- [Max query memory per node cannot be greater than the max query total memory per node](https://github.com/prestodb/presto/issues/11005)
