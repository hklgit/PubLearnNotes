在介绍语言模型之前，先简单回顾一下自然语言处理这个大问题吧。现在自然语言处理的研究绝对是一个非常火热的方向，主要是被当前的互联网发展所带动起来的。在互联网上充斥着大量的信息，主要是文字方面的信息，对这些信息的处理离不开自然语言处理的技术。那么究竟什么是自然语言以及自然语言处理呢？

### 1. 自然语言处理的基本任务

自然语言`Natural Language`其实就是人类语言，自然语言处理（NLP）就是对人类语言的处理，当然主要是利用计算机。自然语言处理是关于计算机科学和语言学的交叉学科，常见的研究任务包括：

- 分词（Word Segmentation或Word Breaker，WB）
- 信息抽取（Information Extraction，IE）：命名实体识别和关系抽取（Named Entity Recognition & Relation Extraction，NER）
- 词性标注（Part Of Speech Tagging，POS）
- 指代消解（Coreference Resolution）
- 句法分析（Parsing）
- 词义消歧（Word Sense Disambiguation，WSD）
- 语音识别（Speech Recognition）
- 语音合成（Text To Speech，TTS）
- 机器翻译（Machine Translation，MT）
- 自动文摘（Automatic Summarization）
- 问答系统（Question Answering）
- 自然语言理解（Natural Language Understanding）
- OCR
- 信息检索（Information Retrieval，IR）

早期的自然语言处理系统主要是基于人工撰写的规则，这种方法费时费力，且不能覆盖各种语言现象。上个世纪80年代后期，机器学习算法被引入到自然语言处理中，这要归功于不断提高的计算能力。研究主要集中在统计模型上，这种方法采用大规模的训练语料（corpus）对模型的参数进行自动的学习，和之前的基于规则的方法相比，这种方法更具鲁棒性。

### 2. 统计语言模型

统计语言模型`Statistical Language Model`就是在这样的环境和背景下被提出来的。它广泛应用于各种自然语言处理问题，如语音识别、机器翻译、分词、词性标注，等等。简单地说，语言模型就是用来计算一个句子的概率的模型，即


```math
P(W_1,W_2,...,W_n)
```

利用语言模型，可以确定哪个词序列的可能性更大，或者给定若干个词，可以预测下一个最可能出现的词语。举个音字转换的例子来说，输入拼音串为nixianzaiganshenme，对应的输出可以有多种形式，如你现在干什么、你西安再赶什么、等等，那么到底哪个才是正确的转换结果呢，利用语言模型，我们知道前者的概率大于后者，因此转换成前者在多数情况下比较合理。再举一个机器翻译的例子，给定一个汉语句子为李明正在家里看电视，可以翻译为Li Ming is watching TV at home、Li Ming at home is watching TV、等等，同样根据语言模型，我们知道前者的概率大于后者，所以翻译成前者比较合理。

那么如何计算一个句子的概率呢？给定句子（词语序列S）


```math
S=W_1,W_2,...,W_n
```
它的概率可以表示为(利用链式法则)：

公式一
```math
P(S)=P(W_1,W_2,...,W_n)=P(W_1)P(W_2|W_1)...P(W_n|W_1,W_2,...,W_n-1)   
```

==备注-链式法则==

```math
P(A,B,C)=P(A)P(B|A)P(C|A,B)
```

由于上式中的参数过多，因此需要近似的计算方法。常见的方法有n-gram模型方法、决策树方法、最大熵模型方法、最大熵马尔科夫模型方法、条件随机域方法、神经网络方法，等等。

### 3. n-gram语言模型

`N-gram`模型也称为n-1阶马尔科夫模型．

#### 3.1 概念

由于上述公式中参数过多，这个概率显然并不好算，我们可以基于马尔科夫假设来近似的计算．

`马尔科夫假设`：
```
"无记忆性"：未来的事件，只取决于有限的历史
```
即当前词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述公式的长度。假设当前词的出现概率仅仅与前面n-1个词相关，因此公式近似为：
```math
P(W_i|W_1,W_2,...,W_i-1) = P(W_i|W_i-n+1,...,W_i-1)  
```
特别地，对于 n 取得较小值的情况：

当 n = 1时(unigram)：

```math
P(W_i|W_1,W_2,...,W_i-1) = P(W_i)  
```
当 n = 2时(bigram)：

```math
P(W_i|W_1,W_2,...,W_i-1) = P(W_i|W_i-1)  
```

当 n = 3时(trigram)：

```math
P(W_i|W_1,W_2,...,W_i-1) = P(W_i|W_i-2,W_i-1)  
```
假设词表的大小为100,000，那么n-gram模型的参数数量为：
```math
100,000^n
```
n越大，模型越准确，也越复杂，需要的计算量越大。最常用的是bigram，其次是unigram和trigram，n取≥4的情况较少．





