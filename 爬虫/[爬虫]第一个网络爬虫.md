为了爬取网站，我们首先需要下载包含有感兴趣数据的网页，该过程一般被称为爬取．

### 1. 下载网页

要想爬取网页，我们首先需要将其下载下来．下面的示例脚本使用Python的`urllib2`模块下载URL:
```
import urllib2

def download(url):
    return urllib2.urlopen(url).read()
```
当传入URL参数时，该函数将会下载网页并返回其HTML．不过，这个代码片段存在一个问题，即当下载网页时，我们可能会遇到一些无法控制的错误，比如请求的页面可能不存在．此时，`urllib2`会抛出异常，然后退出脚本．

下面是一个更健壮的版本，可以捕获异常：
```
import urllib2

def download(url):
    print "downloading:",url
    try:
        content = urllib2.urlopen(url).read()
    except urllib2.URLError as e:
        print "downloading error:",e.reason
        content = None
    return content
```
现在，当出现下载错误时，该函数能够捕获到异常，然后返回Node．但是下载遇到的错误经常是临时性的，比如说服务器过载时返回的`503 Service Unavailable`错误．对于此错误，我们可以尝试重新下载．因为这个服务器问题现在可能已解决．我们只需要确保在发生5xx(服务器端错误)时重新下载即可．

下面是支持重新下载功能的新版本代码：
```
import urllib2

def download(url,repeat=2):
    print "downloading:",url
    try:
        content = urllib2.urlopen(url).read()
    except urllib2.URLError as e:
        print "downloading error:",e.reason
        content = None
        if repeat > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                return download(url, repeat-1)
    return content
```
当下载时遇到5xx错误码时，将会递归调用函数进行重试，此外该函数还增加了一个参数，用于设定重新下载的次数，其默认值为两次．我们在这里限制网页下载的尝试次数，是因为服务器错误可能暂时还没解决．想要测试该函数，可以尝试下载``，该网址会始终返回500错误码．
```
before download ...
downloading: http://httpstat.us/500
downloading error: Internal Server Error
downloading: http://httpstat.us/500
downloading error: Internal Server Error
downloading: http://httpstat.us/500
downloading error: Internal Server Error
download context is None
```
### 2. 用户代理

默认情况下，`urllib2`使用`Python-urllib/2.7`作为用户代理下载网页内容，其中`2.7`是Python的版本号．如果能使用可辨识的用户代理则更好，这样可以避免我们的网络爬虫碰到一些问题．在使用Python默认用户代理的情况下，访问一些网页会被拒绝访问．

为了下载更加可靠，我们需要控制用户代理的设定．下面示例是对上面代码进行了修改，设置了一个默认的用户代理`wswp`：
```
def agentDownload(url,user_agent='wswp',repeat=2):
    print "downloading:",url
    headers = {'user-agent':user_agent}
    request=urllib2.Request(url, headers=headers)
    try:
        content = urllib2.urlopen(request).read()
    except urllib2.URLError as e:
        print "downloading error:",e.reason
        content = None
        if repeat > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                return agentDownload(url, user_agent, repeat-1)
    return content
```

### 3. 链接爬虫

通过跟踪所有链接的方式，我们可以很容易的下载整个网站的页面．但是，这种方法会下载大量我们并不需要的网页．例如，我们想从马蜂窝旅游网站捉取各个目的地详情页，那么此时我们需要下载目的地页即可，而不需要下载其他内容．因此，我们将使用正则表达式来确定需要下载哪些页面．
下面是我们将要爬取的目的地详情页的一个链接：
```
/travel-scenic-spot/mafengwo/10089.html
```
下面是链接爬虫的初始版本：
```
import re
import urllib2

# <a href="/travel-scenic-spot/mafengwo/10186.html" target="_blank">xxx</a>
def get_links(content):
    webpage_regex = re.compile('<a href=\"(/travel-scenic-spot/mafengwo/\d{1,}.html)\" target=\"_blank\">', re.IGNORECASE)
    return webpage_regex.findall(content)


def crawler(seed_url, link_regex):
    crawl_queue = [seed_url]
    while crawl_queue:
        url = crawl_queue.pop()
        print "url:",url
        if "www.mafengwo.cn" not in url:
            url = "http://www.mafengwo.cn/"+url
        content = download(url)
        for link in get_links(content):
            if re.match(link_regex, link):
                crawl_queue.append(link)


def download(url,user_agent='wswp',repeat=2):
    print "downloading:",url
    headers = {'user-agent':user_agent}
    request=urllib2.Request(url, headers=headers)
    try:
        content = urllib2.urlopen(request).read()
    except urllib2.URLError as e:
        print "downloading error:",e.reason
        content = None
        if repeat > 0:
            if hasattr(e, 'code') and 500 <= e.code < 600:
                return download(url, user_agent, repeat-1)
    return content


crawler("http://www.mafengwo.cn/mdd/", "/travel-scenic-spot")
```
输出结果：
```
url: http://www.mafengwo.cn/mdd/
downloading: http://www.mafengwo.cn/mdd/
url: /travel-scenic-spot/mafengwo/10544.html
downloading: http://www.mafengwo.cn//travel-scenic-spot/mafengwo/10544.html
url: /travel-scenic-spot/mafengwo/52088.html
downloading: http://www.mafengwo.cn//travel-scenic-spot/mafengwo/52088.html
url: /travel-scenic-spot/mafengwo/57164.html
downloading: http://www.mafengwo.cn//travel-scenic-spot/mafengwo/57164.html
url: /travel-scenic-spot/mafengwo/17753.html
downloading: http://www.mafengwo.cn//travel-scenic-spot/mafengwo/17753.html
...

```

