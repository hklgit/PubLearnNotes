---
layout: post
author: sjf0115
title: Sqoop 如何导入数据
date: 2017-10-26 19:28:01
tags:
  - Sqoop

categories: Sqoop
permalink: sqoop-base-how-to-use-import
---

### 1. 目标

导入工具将单个表从关系型数据库导入 HDFS。表中的每一行都表示为 HDFS 中的一条单独记录。记录可以存储为文本文件（每行一个记录），或者以二进制表示形式存储为 Avro 或 SequenceFiles。

### 2. 语法

```
$ sqoop import (generic-args) (import-args)
```
Hadoop 通用参数必须位于导入参数之前，导入参数之间没有顺序要求。

通用参数：

参数|描述
---|---
`--connect <jdbc-uri>`| JDBC连接字符串
`--connection-manager <class-name>`| 指定连接类
`--driver <class-name>`| 手动指定 JDBC 驱动类
`--hadoop-mapred-home <dir>`| 覆盖 `$HADOOP_MAPRED_HOME` 路径
`--help`| 帮助
`--password-file`	| 指定包含密码的文件
`-P` | 从命令行读取密码
`--password <password>`	|指定密码
`--username <username>`	|指定用户名
`--verbose`	|打印import过程详细信息

导入控制参数：

参数|描述
---|---
`-m,-num-mappers <n>` | 启动 n 个 Map 并行导入数据，默认是4个
`-table <table-name>` | 关系型数据库表名，数据从该表中获取
`-split-by <column-name>` | 切分导入的数据，一般后面跟主键ID
`-target-dir <dir>` | 导入的 HDFS 路径
`-warehouse-dir <dir>` | 与 `-target-dir` 不能同时使用，指定数据导入的存放目录，适用于 HDFS 导入，不适合导入 Hive 目录
`--delete-target-dir` | 如果导入的路径存在，先删除之前的路径，再导入
`--as-avrodatafile` | 将数据导入到 Avro 数据文件中
`--as-sequencefile` | 将数据导入到 Sequence 数据文件中
`--as-textfile` | 将数据导入到普通文本文件中。默认选项
`--as-parquetfile` | 将数据导入到 Parquet 数据文件中
`-z,--compress` | 启用压缩
`--compression-codec <c>` | Hadoop压缩编码(默认为 gzip)
`-e,--query <statement>` | 从查询结果中导入数据
`--boundary-query <statement>` | 用于创建拆分的边界查询
`--columns <col,col,col…>` | 要导入的表中字段值
`--where <where clause>` | 从关系型数据库导入数据的查询条件
`--fetch-size <n>` | 一次从数据库中读取的条目数
`--inline-lob-limit <n>` | 设定大对象数据类型的最大值
`--direct` | 直接导入模式，使用的是关系型数据库自带的导入导出工具，这样导入会更快
`--null-string <null-string>` | 导入时，将字符串类型的空值设置为指定值
`--null-non-string <null-string>` | 导入时，将非字符串类型的空值设置为指定值

### 3. 连接数据库服务器

Sqoop 旨在将数据库中的表导入 HDFS。为此，你必须指定一个描述如何连接到数据库的连接字符串。连接字符串类似于URL，并使用 `--connect` 参数传递给 Sqoop。下面示例描述了要连接的服务器和数据库（employees），以及端口（3306）：
```
$ sqoop import --connect jdbc:mysql://127.0.0.1:3306/employees
```
上面示例将连接到主机 `127.0.0.1` 上名为 `employees` 的 MySQL 数据库。如果你打算将 Sqoop 与分布式 Hadoop 集群一起使用，不能使用URL `localhost`。你提供的连接字符串将用于整个 MapReduce 集群中的任务节点，如果指定 `localhost`，则每个节点将连接到本机自己的数据库（或者根本没有数据库）。相反，你应该使用所有远程节点都可以看到的数据库主机的完整主机名或IP地址。

你可能需要先对数据库进行身份验证，然后才能访问它。你可以使用 `--username` 为数据库提供用户名。

`--password` 参数是不安全的，因为其他用户可能能够通过 ps 等程序的输出从命令行参数读取密码。推荐使用 `-P` 参数。

### 3. 选择导入的数据

Sqoop 通常以表为中心导入数据。使用 `--table` 参数选择要导入的表。例如， `--table employees`。此参数也可以是数据库中的视图或其他类似表的实体。

默认情况下，表中所有列都会导入。导入的数据以 '自然顺序' 写入HDFS，也就是说，包含A，B和C列的表会导入数据后如下所示：
```
A1,B1,C1
A2,B2,C2
...
```
你可以使用 `--columns` 参数选择指定的列并控制它们的顺序。值为以逗号分隔的列的列表。例如：
```
--columns "name,employee_id,jobtitle"
```

Sqoop 不需要每次都导入整张表。例如，可以指定仅导入表的部分列。你可以通过向 import 语句添加 WHERE 子句（使用 `--where`参数）来控制导入哪些行。例如，昨天已经将 id 为 0 ~ 400的记录导入，而今天又增加了许多记录，那么导入时在查询语句中加入子句 `--where "id > 400"`，就可以实现导入id值大于400的行。默认情况下，Sqoop 生成 `SELECT <column list> FROM <table name>` 形式的语句。

默认情况下，Sqoop 将使用 `select min(<split-by>), max(<split-by>) from <table name>` 来创建拆分的边界。在某些情况下，此查询不是最佳查询，因此你可以使用 `--boundary-query` 参数指定任意返回两个数字列的任意查询。

### 4. 自由查询模式导入数据

Sqoop 还可以导入任意 SQL 查询的结果集。你可以使用 `--query` 参数指定 SQL 语句，来代替 `--table`， `- column` 和 `--where` 参数。自由查询格式导入数据时，必须使用 `--target-dir` 指定目标目录。

如果要并行导入查询结果，则每个 Map 任务都需要执行查询的副本，并通过 Sqoop 推断的边界条件对结果进行分区。查询语句 `WHERE` 后必须包含 `$CONDITIONS`，每个 Sqoop 进程将替换为唯一的条件表达式。你还必须指定 `--split-by` 选择拆分列。

Example:
```
$ sqoop import \
  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \
  --split-by a.id --target-dir /user/foo/joinresults
```
或者，通过使用 `-m 1` 指定为一个 Map 任务，可以执行一次查询并串行导入（此时不在需要使用 `--split-by` 参数）：
```
$ sqoop import \
  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \
  -m 1 --target-dir /user/foo/joinresults
```

> 自由查询模式下必须使用 `--target-dir` 指定目标目录，查询语句 `WHERE` 后必须包含 `$CONDITIONS`，并行导入时还必须指定 `--split-by` 选择拆分列（除非使用 `-m 1` 参数指定为一个 Map 任务）。

如果你的查询使用双引号包裹，则必须使用 `\$CONDITIONS` 来代替 `$CONDITIONS`，以禁止 shell 将其视为 shell 变量。例如，双引号查询可能如下所示：
```
"SELECT * FROM x WHERE a ='foo'AND \$CONDITIONS"
```

> 在当前版本的 Sqoop 中使用自由格式查询的功能仅限于简单查询，其中 WHERE 子句中不能有不明确的投影和 OR 条件。使用复杂查询（例如具有子查询或联接的查询导致模糊投影）可能会导致意外结果。

### 5. 导入控制

默认情况下，导入过程使用 JDBC。某些数据库可以使用数据库特定的数据转移工具以更高性能的方式执行导入。例如，MySQL 提供了 `mysqldump` 工具，可以非常快速地将数据从 MySQL 导出到其他系统。通过提供 `--direct` 参数，你可以指定 Sqoop 尝试使用直接导入方式。此方式可能比使用 JDBC 具有更高的性能。

默认情况下，Sqoop 会将 `foo` 表导入 HDFS 中主目录下的 `foo` 目录中。例如，如果你的用户名是 `smartsi`，则导入工具将数据写入到 `/user/smartsi/foo/（files）`。你可以使用 `--warehouse-dir` 参数调整导入的父目录。例如：
```
$ sqoop import --connnect <connect-str> --table foo --warehouse-dir /shared \
...
```
此命令将数据写入到 `/shared/foo/` 目录下文件中。

你还可以显式指定目标目录，如下所示：
```
$ sqoop import --connnect <connect-str> --table foo --target-dir /dest \
...
```
这会将数据导入到 `/dest` 目录下文件中。

> --target-dir与--warehouse-dir不兼容。

使用直接模式时，你可以指定传递给底层工具的其他参数。如果参数 `--` 在命令行中给出，则后续的参数会发送给底层工具。例如，下面调整 `mysqldump` 使用的字符集：
```
$ sqoop import --connect jdbc:mysql://server.foo.com/db --table bar \
    --direct -- --default-character-set=latin1
```
默认情况下，会导入到一个新的目标位置。如果目标目录已存在于 HDFS 中，Sqoop 将拒绝导入并覆盖该目录的内容。如果使用 `--append` 参数，Sqoop会将数据导入临时目录，然后以不与该目录中的现有文件名冲突的方式将文件重命名为普通目标目录。

### 6. 控制并发度

Sqoop 可以从大多数数据库并行导入数据。你可以使用 `-m` 或 `--num-mappers` 参数指定要用于执行导入的 Map 任务（并行进程）的个数。默认情况下，会启动四个任务。某些数据库可能会通过将此值增加到8或16来提高性能。如果任务以串行方式运行，可能会增加执行导入所需的时间。同样，不要将并行度提高到超过数据库可以合理支持的程度。100个客户端并发连接到数据库可能会增加数据库服务器上的负载到性能受损的程度。

执行并行导入时，Sqoop 需要一个可以分割工作负载的标准。Sqoop 使用拆分列来分割工作负载。默认情况下，Sqoop 识别表中的主键列（如果存在）并将其用作拆分列。从数据库中检索出拆分列的最小值和最大值，Map 任务在总范围的大小均匀的组件上运行。例如，如果你的表的主键列id为最小值为0以及最大值为1000，并且 Sqoop 使用4个任务，则 Sqoop 将运行四个进程，每个进程执行如下形式：
```sql
SELECT *
FROM sometable
WHERE id >= lo AND id < hi,
with (lo, hi) set to (0, 250), (250, 500), (500, 750), and (750, 1001)
```
如果主键的实际值在其范围内分布不均匀，则可能导致任务出现数据倾斜。你应该使用 `--split-by` 参数明确选择一个不同的列。例如，`--split-by employee_id`。Sqoop 目前无法拆分多列索引。如果表没有索引列，或者具有联合主键，则必须手动选择拆分列。

如果表没有定义主键并且未提供 `--split-by <col>`，则导入将失败，除非使用 `--num-mappers 1` 选项或使用 `-autoreset-to-one-mapper` 选项将 Mapper 的数量显式设置为1。选项 `--autoreset-to-one-mapper` 通常与 `import-all-tables` 工具一起使用，以自动处理模式中没有主键的表。

### 7. 增量导入

Sqoop提供了一种增量导入模式，可用于仅检索比先前导入的行更新的行。

以下参数控制增量导入：

参数|说明
---|---
`--check-column (col)` | 指定在确定要导入哪些行时要检查的列。（该列不能为CHAR/NCHAR/VARCHAR/VARNCHAR/LONGVARCHAR/LONGNVARCHAR类型）
`--incremental (mode)` | 指定 Sqoop 如何确定哪些行是新的。模式的合法值包括 `append` 和 `lastmodified`
`--last-value (value)` | 指定上一次导入时检查列的最大值

Sqoop 支持两种类型的增量导入：`append` 和 `lastmodified`。你可以使用 `--incremental` 参数指定要执行的增量导入的模式。

如果有不断的新行添加同时伴随着行ID值是递增的，这时你应该选用 `append` 模式。

你可以使用 `--check-column` 指定包含行ID的列。Sqoop 会导入检查列值大于 `--last-value` 指定值的行。

Sqoop 支持的另一个模式为 `lastmodified` 模式。你应该在数据源表的行可能会更新时使用此项，每次更新时都会将 `last-modified` 列的值设置为当前时间戳。检查列包含的时间戳比 `--last-value` 指定的时间戳更新的行会被导入。

在增量导入结束时，为后续导入指定的 `--last-value` 的值要打印到屏幕上。运行后续导入时，应以这种方式指定 `--last-value`，以确保仅导入新数据。这是通过将增量导入创建为已保存作业来自动处理的，这是执行定期增量导入的首选机制。有关详细信息，请参阅本文档后面有关已保存作业的部分。

### 8. 文件格式

你可以使用以下两种文件格式导入数据：分隔文本（delimited Text）或 `SequenceFiles`。

分隔文本是默认导入格式。你还可以使用 `--as-textfile` 参数显式指定它。此参数将每个记录的基于字符串的表示形式写入输出文件，并在各列和行之间使用分隔符。这些分隔符可以是逗号，制表符或其他字符。以下是基于文本的示例导入的结果：
```
1,here is a message,2010-05-01
2,happy new year!,2010-01-01
3,another message,2009-11-12
```
分隔文本适用于大多数非二进制数据类型。它还很容易支持其他工具（如Hive）的进一步操作。

`SequenceFiles` 是一种二进制格式，用于将记录存储在自定义特定于记录的数据类型中。

`Avro` 数据文件是一种紧凑，高效的二进制格式，可提供与其他编程语言编写的应用程序进行交互操作。`Avro` 还支持版本控制，因此当，例如从表中添加或删除列时，先前导入的数据文件可以与新的数据文件一起处理。

默认情况下，数据不会压缩。你可以使用 `deflate（gzip）` 算法和 `-z` 或 `--compress` 参数对数据进行压缩，或使用 `--compression-codec` 参数指定 Hadoop 压缩编解码器。这适用于 SequenceFile，文本和 Avro 文件。

### 9. 输出格式

导入分隔文件时，分隔符的选择非常重要。分隔符出现在字符串类型的字段中可能会导致后续对导入数据进行解析出现问题。例如，导入字符串 `Hello, pleased to meet you` 时，字段结束分隔符不应该设置为逗号。

分隔符可以指定为：
- 一个字符（ `--fields-terminated-by X`）
- 转义字符（`--fields-terminated-by \t`）。支持的转义字符有：`\b`，`\n`，`\r`，`\t`，`\"`，`\\'`，`\\`，`\0`。
- UTF-8的八进制值。应该是 `\0ooo` 的形式，其中 `ooo` 是八进制值。例如，`-fields-terminated-by \001`将产生 `^A` 字符。
- UTF-8的十六进制值。应该是 `\0xhhh` 的形式，其中 `hhh` 是十六进制值。例如， `-fields-terminated-by \0x10` 将产生回车符。

字段默认分隔符是逗号（`，`），记录分隔符为换行符（`\n`）。请注意，如果导入的数据库记录中包含逗号或换行符，则会导致模糊/不可分类的记录。为了明确解析，必须启用两者。例如，通过 `--mysql-delimiters`。

如果无法显示明确的分隔符，则使用封闭和转义字符。（可选）封闭和转义字符的组合将允许明确解析行。 例如，假设数据集的一列包含以下值：
```
Some string, with a comma.
Another "string with quotes"
```
以下参数将提供可以明确解析的分隔符：
```
$ sqoop import --fields-terminated-by , --escaped-by \\ --enclosed-by '\"' ...
```
> 为了防止shell破坏封闭字符，我们将该参数本身包含在单引号中。

上述参数应用于上述数据集的结果为：
```
"Some string, with a comma.","1","2","3"...
"Another \"string with quotes\"","4","5","6"...
```






> Sqoop版本：1.4.6

参考：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_syntax
