---
layout: post
author: sjf0115
title: Sqoop 如何导入数据
date: 2017-10-26 19:28:01
tags:
  - Sqoop

categories: Sqoop
permalink: sqoop-base-how-to-use-import
---

### 1. 目标

导入工具将单个表从关系型数据库导入 HDFS。表中的每一行都表示为 HDFS 中的一条单独记录。记录可以存储为文本文件（每行一个记录），或者以二进制表示形式存储为 Avro 或 SequenceFiles。

### 2. 语法

```
$ sqoop import (generic-args) (import-args)
```
Hadoop 通用参数必须位于导入参数之前，导入参数之间没有顺序要求。

通用参数：

参数|描述
---|---
`--connect <jdbc-uri>`| JDBC连接字符串
`--connection-manager <class-name>`| 指定连接类
`--driver <class-name>`| 手动指定 JDBC 驱动类
`--hadoop-mapred-home <dir>`| 覆盖 `$HADOOP_MAPRED_HOME` 路径
`--help`| 帮助
`--password-file`	| 指定包含密码的文件
`-P` | 从命令行读取密码
`--password <password>`	|指定密码
`--username <username>`	|指定用户名
`--verbose`	|打印import过程详细信息

导入控制参数：

参数|描述
---|---
`-m,-num-mappers <n>` | 启动 n 个 Map 并行导入数据，默认是4个
`-table <table-name>` | 关系型数据库表名，数据从该表中获取
`-split-by <column-name>` | 切分导入的数据，一般后面跟主键ID
`-target-dir <dir>` | 导入的 HDFS 路径
`-warehouse-dir <dir>` | 与 `-target-dir` 不能同时使用，指定数据导入的存放目录，适用于 HDFS 导入，不适合导入 Hive 目录
`--delete-target-dir` | 如果导入的路径存在，先删除之前的路径，再导入
`--as-avrodatafile` | 将数据导入到 Avro 数据文件中
`--as-sequencefile` | 将数据导入到 Sequence 数据文件中
`--as-textfile` | 将数据导入到普通文本文件中。默认选项
`--as-parquetfile` | 将数据导入到 Parquet 数据文件中
`-z,--compress` | 启用压缩
`--compression-codec <c>` | Hadoop压缩编码(默认为 gzip)
`-e,--query <statement>` | 从查询结果中导入数据
`--boundary-query <statement>` | 用于创建拆分的边界查询
`--columns <col,col,col…>` | 要导入的表中字段值
`--where <where clause>` | 从关系型数据库导入数据的查询条件
`--fetch-size <n>` | 一次从数据库中读取的条目数
`--inline-lob-limit <n>` | 设定大对象数据类型的最大值
`--direct` | 直接导入模式，使用的是关系型数据库自带的导入导出工具，这样导入会更快
`--null-string <null-string>` | 导入时，将字符串类型的空值设置为指定值
`--null-non-string <null-string>` | 导入时，将非字符串类型的空值设置为指定值

### 3. 连接数据库服务器

Sqoop 旨在将数据库中的表导入 HDFS。为此，你必须指定一个描述如何连接到数据库的连接字符串。连接字符串类似于URL，并使用 `--connect` 参数传递给 Sqoop。下面示例描述了要连接的服务器和数据库（employees），以及端口（3306）：
```
$ sqoop import --connect jdbc:mysql://127.0.0.1:3306/employees
```
上面示例将连接到主机 `127.0.0.1` 上名为 `employees` 的 MySQL 数据库。如果你打算将 Sqoop 与分布式 Hadoop 集群一起使用，不能使用URL `localhost`。你提供的连接字符串将用于整个 MapReduce 集群中的任务节点，如果指定 `localhost`，则每个节点将连接到本机自己的数据库（或者根本没有数据库）。相反，你应该使用所有远程节点都可以看到的数据库主机的完整主机名或IP地址。

你可能需要先对数据库进行身份验证，然后才能访问它。你可以使用 `--username` 为数据库提供用户名。

`--password` 参数是不安全的，因为其他用户可能能够通过 ps 等程序的输出从命令行参数读取密码。推荐使用 `-P` 参数。

### 3. 选择导入的数据

Sqoop 通常以表为中心导入数据。使用 `--table` 参数选择要导入的表。例如， `--table employees`。此参数也可以是数据库中的视图或其他类似表的实体。

默认情况下，表中所有列都会导入。导入的数据以 '自然顺序' 写入HDFS，也就是说，包含A，B和C列的表会导入数据后如下所示：
```
A1,B1,C1
A2,B2,C2
...
```
你可以使用 `--columns` 参数选择指定的列并控制它们的顺序。值为以逗号分隔的列的列表。例如：
```
--columns "name,employee_id,jobtitle"
```

你可以通过向 import 语句添加 SQL WHERE 子句来控制导入哪些行。默认情况下，Sqoop 生成 `SELECT <column list> FROM <table name>` 形式的语句。你可以使用 `--where` 参数附加上 `WHERE` 子句。例如： `--where "id > 400"`。只会导入id值大于400的行。

默认情况下，Sqoop 将使用 `select min(<split-by>), max(<split-by>) from <table name>` 来创建拆分的边界。在某些情况下，此查询不是最佳查询，因此你可以使用 `--boundary-query` 参数指定任意返回两个数字列的任意查询。

### 4. 自由查询格式导入数据

Sqoop 还可以导入任意 SQL 查询的结果集。你可以使用 `--query` 参数指定 SQL 语句，来代替 `--table`， `- column` 和 `--where` 参数。自由查询格式导入数据时，必须使用 `--target-dir` 指定目标目录。

如果要并行导入查询结果，则每个 Map 任务都需要执行查询的副本，并通过 Sqoop 推断的边界条件对结果进行分区。查询语句中必须包含令牌 `$CONDITIONS`，每个 Sqoop 进程将替换为唯一的条件表达式。你还必须使用 `--split-by` 选择拆分列。

Example:
```
$ sqoop import \
  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \
  --split-by a.id --target-dir /user/foo/joinresults
```
或者，通过使用 `-m 1` 指定为一个 Map 任务，可以执行一次查询并串行导入：
```
$ sqoop import \
  --query 'SELECT a.*, b.* FROM a JOIN b on (a.id == b.id) WHERE $CONDITIONS' \
  -m 1 --target-dir /user/foo/joinresults
```

如果你的查询使用双引号包裹，则必须使用 `\$CONDITIONS` 来代替 `$CONDITIONS`，以禁止 shell 将其视为 shell 变量。例如，双引号查询可能如下所示：
```
"SELECT * FROM x WHERE a ='foo'AND \$CONDITIONS"
```

> 在当前版本的 Sqoop 中使用自由格式查询的功能仅限于简单查询，其中 WHERE 子句中不能有不明确的投影和 OR 条件。使用复杂查询（例如具有子查询或联接的查询导致模糊投影）可能会导致意外结果。

### 5. 控制导入过程

默认情况下，导入过程使用 JDBC。某些数据库可以使用数据库特定的数据转移工具以更高性能的方式执行导入。例如，MySQL 提供了 `mysqldump` 工具，可以非常快速地将数据从 MySQL 导出到其他系统。通过提供 `--direct` 参数，你可以指定 Sqoop 尝试使用直接导入方式。此方式可能比使用 JDBC 具有更高的性能。

默认情况下，Sqoop 会将 `foo` 表导入 HDFS 中主目录下的 `foo` 目录中。例如，如果你的用户名是 `smartsi`，则导入工具将数据写入到 `/user/smartsi/foo/（files）`。你可以使用 `--warehouse-dir` 参数调整导入的父目录。例如：
```
$ sqoop import --connnect <connect-str> --table foo --warehouse-dir /shared \
...
```
此命令将数据写入到 `/shared/foo/` 目录下文件中。

你还可以显式指定目标目录，如下所示：
```
$ sqoop import --connnect <connect-str> --table foo --target-dir /dest \
...
```
这会将数据导入到 `/dest` 目录下文件中。

> --target-dir与--warehouse-dir不兼容。

使用直接模式时，你可以指定传递给底层工具的其他参数。如果参数 `--` 在命令行中给出，则后续的参数会发送给底层工具。例如，下面调整 `mysqldump` 使用的字符集：
```
$ sqoop import --connect jdbc:mysql://server.foo.com/db --table bar \
    --direct -- --default-character-set=latin1
```
默认情况下，会导入到一个新的目标位置。如果目标目录已存在于 HDFS 中，Sqoop 将拒绝导入并覆盖该目录的内容。如果使用 `--append` 参数，Sqoop会将数据导入临时目录，然后以不与该目录中的现有文件名冲突的方式将文件重命名为普通目标目录。

### 6. 控制并发度

Sqoop 可以从大多数数据库并行导入数据。你可以使用 `-m` 或 `--num-mappers` 参数指定要用于执行导入的 Map 任务（并行进程）的个数。默认情况下，会启动四个任务。某些数据库可能会通过将此值增加到8或16来提高性能。如果任务以串行方式运行，可能会增加执行导入所需的时间。同样，不要将并行度提高到超过数据库可以合理支持的程度。100个客户端并发连接到数据库可能会增加数据库服务器上的负载到性能受损的程度。

执行并行导入时，Sqoop 需要一个可以分割工作负载的标准。Sqoop 使用拆分列来分割工作负载。默认情况下，Sqoop 识别表中的主键列（如果存在）并将其用作拆分列。从数据库中检索出拆分列的最小值和最大值，Map 任务在总范围的大小均匀的组件上运行。例如，如果你的表的主键列id为最小值为0以及最大值为1000，并且 Sqoop 使用4个任务，则 Sqoop 将运行四个进程，每个进程执行如下形式：
```sql
SELECT *
FROM sometable
WHERE id >= lo AND id < hi,
with (lo, hi) set to (0, 250), (250, 500), (500, 750), and (750, 1001)
```
如果主键的实际值在其范围内分布不均匀，则可能导致任务出现数据倾斜。你应该使用 `--split-by` 参数明确选择一个不同的列。例如，`--split-by employee_id`。Sqoop 目前无法拆分多列索引。如果表没有索引列，或者具有联合主键，则必须手动选择拆分列。

如果表没有定义主键并且未提供 `--split-by <col>`，则导入将失败，除非使用 `--num-mappers 1` 选项或使用 `-autoreset-to-one-mapper` 选项将 Mapper 的数量显式设置为1。选项 `--autoreset-to-one-mapper` 通常与 `import-all-tables` 工具一起使用，以自动处理模式中没有主键的表。

> Sqoop版本：1.4.6

参考：http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html#_syntax
