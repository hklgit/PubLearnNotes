---
layout: post
author: sjf0115
title: Flink 使用Flink进行高吞吐，低延迟和Exact-Once语义流处理
date: 2018-09-02 14:54:01
tags:
  - Flink
  - Flink 内部原理

categories: Flink
permalink: high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink
---


在本文中，我们将深入探讨Flink新颖的检查点机制是如何工作的，以及它是如何取代旧架构以实现流容错和恢复。我们在各种类型的流处理应用程序上对Flink性能进行测试，并通过在Apache Storm（一种广泛使用的低延迟流处理器）上运行相同的实验来进行对比。

### 1. 流式架构的演变

保证容错和高性能流处理是比较难的。在批处理中，当作业失败时，可以简单地重新运行作业的失败部分以重新创建丢失的结果。这在批处理中是可行的，因为文件可以从头到尾重放。但是在连续流处理中却不是如此。数据流是去穷无尽的，不具有开始点和结束点。带有缓冲的数据流可以进行重播一小段数据，但从最开始重播数据流是不切实际的（流处理作业可以运行数月）。此外，与仅具有输入和输出的批处理作业相比，流式计算是有状态的。这意味着除了输出之外，系统还需要备份和恢复算子状态。由于问题的复杂性，在开源生态系统中有许多容错方法。

用于容错机制也对整个框架的架构有深远的影响。很难实现以插件的形式将不同的容错机制整合到现有框架中。因此，在我们选择一个流处理框架时，容错策略非常重要。

下面我们去了解一下流处理架构的几种容错方法，从`记录确认`到`微批处理`，`事务更新`和`分布式快照`。我们将从以下几个维度讨论不同方法的优缺点，最终选出融合不同方法优点适合流处理程序的组合方法：
- Exactly-once 语义保证：故障后有状态算子的状态应正确恢复。
- 低延迟：延迟越低越好。许多应用程序需要亚秒级延迟。
- 高吞吐量：随着数据速率的增长，通过管道推送大量数据至关重要。
- 强大的计算模型：框架应该提供一种编程模型，该模型不限制用户并允许各种应用程序在没有故障的情况下容错机制保证低开销。
- 流量控制：来自处理速度慢的算子的背压应该由系统和数据源自然吸收，以避免因消费缓慢而导致崩溃或降低性能。

我们遗漏了一个共同特征，即失败后的快速恢复，不是因为它不重要，而是因为（1）所有介绍的系统都能够基于完全并行进行恢复，以及（2）在有状态的应用程序中，状态恢复的瓶颈通常在于存储而不是计算框架。

### 2. 记录确认机制(Apache Storm)

虽然流处理已经在诸如金融等行业中广泛使用多年，但最近流处理才成为大数据基础设施的一部分。开源框架的可用性一直在推动着流处理的发展。也许在开源世界中第一个广泛使用的大规模流处理框架是Apache Storm。Storm使用[上游备份和记录确认机制](http://storm.apache.org/releases/current/Guaranteeing-message-processing.html)来保证在失败后重新处理消息。请注意，Storm不保证状态一致性，任何可变状态处理都委托给用户处理（Storm的Trident API确保状态一致性，将在下一节中介绍）。

记录确认机制的工作方式如下：算子(Operator)处理的每条记录都会向前一个算子发回一个已经处理过的确认。Topology 的 Source 会保留它产生的所有记录的一个备份来处理失败情况。当 Source 中一条记录所产生的所有派生记录都被整个 Topology 处理完成，Source 节点就可以删除其备份；当系统出现部分 Fail 情况，例如一条记录并没有收到其下游的派生记录的确认，Source 就会重新发送该记录到下游的 Topology 以便重新进行计算。 这种处理机制可以保证整个处理过程不会丢失数据，但很有可能导致同一条记录被多次发送到下游进行处理（我们称之为“at least once”）。 Storm 使用一种巧妙的机制来实现这种容错方式，每个源记录只需要几个字节的存储来跟踪确认。 Twitter Heron 保持与 Storm 相同的确认机制，但提高了记录重放的效率（从而提高了恢复时间和整体吞吐量）。

### 3. 微批处理(Apache Storm Trident, Apache Spark Streaming)

Storm和先前的流处理系统不能满足一些对大规模应用程序至关重要的要求，特别是高吞吐量，快速并行恢复以及托管状态的Exactly-once语义。

容错流式架构的下一个发展阶段是微批处理或离散化流的想法。这个想法非常简单：为了克服处理和缓冲记录的持续计算模型所带来的记录级别同步的复杂性和开销（译者注：流式处理系统中的算子都是在记录级别进行计算同步和容错，由此带来了在记录级别上进行处理的复杂和开销），连续计算分解为一系列小的原子批处理作业（称为微批次）。每个微批次都可能会成功或失败。如果发生故障，可以简单地重新计算最新的微批次。

![]()

微批处理是一种可以应用在能够进行数据流计算的现有引擎之上的技术。例如，可以在批处理引擎（例如，Spark）之上应用微批处理以提供流功能（这是Spark Streaming背后的基本机制），并且它还可以应用于流引擎之上（例如，Storm）提供 Exactly-once 语义保证和状态恢复（这是Storm Trident背后的基本机制）。在 Spark Streaming 中，每个微批次计算都是一个 Spark 作业，而在 Trident 中，每个微批次中的所有记录都会被合并为一个大型记录。

基于微批处理的系统可以实现上面列出的很多的需求（Exactly-once语义保证，高吞吐量），但还有很多不足之处：
- 编程模型：为了实现其目标，例如，Spark Streaming 将编程模型从流式更改为微批处理。这意味着用户不能再以任意时间而只能在检查点间隔的倍数上窗口化数据，并且模型不支持许多应用程序所需的基于计数或会话的窗口。这些都是应用程序开发人员关注的问题。具有可以改变状态的持续计算的纯流模型为用户提供了更大的灵活性。
- 流量控制：使用基于时间划分批次的微批次架构仍然具有背压效应的问题。如果微批处理在下游操作中（例如，由于计算密集型算子处理不过来或向外部存储数据比较缓慢）比在划分批次的算子（通常是源）中花费更长时间，则微批次将花费比配置更长的时间（译者注：下游算子处理速度跟不上划分批次算子的速度）。这导致越来越多的批次排队，或者导致微批量增加。
- 延迟：微批处理显然将作业的延迟限制为微批处理的延迟。虽然亚秒级的批处理延迟对于简单应用程序是可以接受的，但是具有多个网络Shuffle的应用程序很容易将延迟时间延长到数秒。

微批处理模型的最大局限可能是它连接了两个不应连接的概念：应用程序定义的窗口大小和系统内部恢复间隔。假设一个程序（下面示例是Flink代码）每5秒聚合一次记录：
```
dataStream
    .map(transformRecords)
    .groupBy("sessionId")
    .window(Time.of(5, TimeUnit.SECONDS))
    .sum("price")
```
这些应用非常适合微批处理模型。系统累积5秒的数据，对它们求和，并在流上进行一些转换后进行聚合计算。下游应用程序可以直接消费上述5秒聚合后的结果，例如在仪表板上显示。但是，现在假设[背压](http://smartsi.club/2018/02/11/how-flink-handles-backpressure/)开始起作用（例如，由于计算密集型的 transformRecords 函数），或者 devops 团队决定通过将时间间隔增加到10秒来控制作业的吞吐量。然后，微批次大小变的不可控制（在出现背压情况下），或者直接变为10秒（第二种情况）。这意味着下游应用程序（例如，包含最近5秒统计的 Web 仪表板）读取的聚合结果是错误的，下游应用程序需要自己处理此问题。

微批处理可以实现高吞吐量和Exactly-Once语义保证，但是当前的实现是以抛弃低延迟，流量控制和纯流式编程模型为代价实现上述目标的。显而易见的问题是，是否有两全其美的办法：保持持续计算模型的所有优势，同时还能保证Exactly-Once语义并提供高吞吐量。后面讨论的后流式架构实现了这种组合，并将微批处理作为流式处理的基本模型。

通常，微批处理被认为是一次处理一条记录的替代方法。这是一种错误的认识：持续计算不需要一次只处理一条记录。实际上，所有精心设计的流处理系统（包括下面讨论的Flink和Google Dataflow）在通过网络传输之前会缓冲许多记录，同时又具备连续的处理能力。

### 4. 事务更新(Google Cloud Dataflow)

在保留持续算子模型（低延迟，背压容错，可变状态等）的优势的同时又保证Exactly-Once处理语义的一种强大而又优雅的方法是原子性地记录需要处理的数据并更新到状态中。失败后，可以从日志中重新恢复状态以及需要处理的记录。

例如，在Google Cloud Dataflow中实现了此概念。系统将计算抽象为一次部署并长期运行的连续算子的DAG。在Dataflow中，shuffle是流式传输的，结果不需要物化（译者注：数据的计算结果放在内存中）。这为低延迟提供了一种自然的流量控制机制，因为中间过程的缓冲可以缓解背压，直到反压到数据源（基于Pull模式的数据源，例如Kafka消费者可以处理这个问题）。该模型还提供了一个优雅的流编程模型，可以提供更丰富的窗口而不是简单的基于时间的窗口以及可以更新到长期可变的状态中。值得注意的是，流编程模型包含微批处理模型。

例如，下面Google Cloud Dataflow程序（请参阅此处）会创建一个会话窗口，如果某个key的事件没有在10分钟内到达，则会触发该会话窗口。在10分钟后到达的数据将会启动一个新窗口。

```
PCollection<String> items = ...;
PCollection<String> session_windowed_items = items.apply(
    Window.<String>into(Sessions.withGapDuration(Duration.standardMinutes(10))))
```

这在流式传输模型中很容易实现，但在微批处理模型中却很难实现，因为窗口不对应于固定的微批量大小。

这种架构中的容错工作原理如下。通过算子的每个中间记录与更新的状态以及后续产生的记录一起创建一个提交记录，该记录以原子性的方式追加到事务日志或插入到数据库中。在失败的情况下，重放部分数据库日志来持续的恢复计算状态，以及重放丢失的记录。

Apache Samza遵循类似的方法，但只能提供At-Least-Once语义保证，因为它使用Apache Kafka作为后台存储。Kafka（现在）不提供事务编写器，因此对状态和后续产生的流记录的更新不能作为原子事务一起提交。

事务更新体系结构具有许多优点。事实上，它实现了我们在本文开头提出的所有需求。该体系结构的基础是能够频繁地写入具有高吞吐量的分布式容错存储系统中。分布式快照（在下一节中进行了解释）将拓扑的状态作为一个整体进行快照，从而减少了对分布式存储的写入量和频率。

### 5. 分布式快照(Apache Flink)

提供 Exactly-Once 语义保证实际上可以归结为确定当前流式计算所处的状态（包括正在处理中记录以及算子状态），生成该状态的一致性快照，并将该快照存储在持久存储中。如果可以经常执行此操作，则从故障中恢复意味着仅从持久存储中恢复最新快照，将流数据源（例如，Apache Kafka）回退到生成快照的时间点再次'播放'。Flink对应的算法可以参阅[本文](https://arxiv.org/abs/1506.08603); 在下文中，我们会给出一个简短的总结。

Flink的快照算法基于Chandy和Lamport于1985年设计的一种算法，用于生成分布式系统当前状态的一致性快照（详细介绍请参阅[此处](http://blog.acolyer.org/2015/04/22/distributed-snapshots-determining-global-states-of-distributed-systems/)），不会丢失信息且不会记录重复项。Flink使用的是Chandy Lamport算法的一个变种，定期生成正在运行的流拓扑的状态快照，并将这些快照存储到持久存储（例如，存储到HDFS或内存中文件系统）。这些检查点的频率是可配置的。

这类似于微批处理方法，两个检查点之间的所有计算都作为一个原子整体，要么全部成功，要么全部失败。然而，只有这一点的相似之处。Chandy Lamport算法的一个重要特点是我们不必在流处理中按下'暂停'按钮（译者注：等待检查点完成之后）来调度下一个微批次。相反，常规数据处理始终保持运行，数据到达就会处理，而检查点发生在后台。引用原始论文：
```
全局状态检测算法会被设计在基础计算上：它们必须同时运行，但不能改变基础计算。
```
因此，这种架构融合了连续算子模型（低延迟，流量控制和真正的流编程模型），高吞吐量，Chandy-Lamport算法提供的的Exactly-Once语义保证的优点。除了备份有状态计算的状态（其他容错机制也需要这样做）之外，这种容错机制几乎没有其他开销。对于小状态（例如，计数或其他统计），备份开销通常可以忽略不计，而对于大状态，检查点间隔会在吞吐量和恢复时间之间进行权衡。

最重要的是，该架构将应用程序开发与流量控制和吞吐量控制分开。更改快照间隔对流作业的结果完全没有影响，因此下游应用程序可以放心地依赖于接收到的正确结果。

Flink的检查点机制基于流经算子和渠道的 'barrier'（认为是Chandy Lamport算法中的一种'标记'）实现。Flink的检查点的描述改编自[Flink文档](http://smartsi.club/2018/01/24/flink-data-streaming-fault-tolerance/)。

'Barrier' 在 Source 节点中被注入到普通流数据中（例如，如果使用Apache Kafka作为源，'barrier' 与偏移量对齐），并且作为数据流的一部分与数据流一起流过DAG。'barrier' 将记录分为两组：当前快照的一部分（'barrier' 表示检查点的开始），以及属于下一个快照的那些组。

![]()

'Barrier' 流向下游并在通过算子时触发状态快照。算子首先将 'barrier' 与所有流入的流分区（如果算子具有多个输入）对齐，会缓存较快的分区数据（上游来源较快的流分区将被缓冲数据以等待来源较慢的流分区）。当算子从每个输入流中都收到 'barrier' 时，会检查其状态（如果有）并写到持久存储中。一旦完成状态写检查，算子就将 'barrier' 向下游转发。请注意，在此机制中，如果算子支持，则状态写检查既可以是异步（在写入状态时继续处理），也可以是增量（仅写入更改）。

![]()

一旦所有数据接收器（Sink）都收到 'barrier'，当前检查点就完成了。故障恢复意味着只需恢复最新的检查点状态，并从最后记录的 'barrier' 对应的偏移量重新重放数据源。分布式快照在我们在本文开头所要达到的所有需求中得分很高。它们实现了高吞吐量的Exactly-Once语义保证，同时还保留了连续算子模型以及低延迟和自然流量控制。

### 6. 结论

下表总结了我们讨论的每个体系结构如何支持这些功能。

|记录确认机制|微批次|事务更新|分布式快照
---|---|---|---|---
语义保证|At Least Once|Exactly Once|Exactly One|Exactly One
延迟|非常低|高|低（事务延迟）|非常低
吞吐量|低|高|中到高（取决于分布式事务存储的吞吐量）|高
计算模型|流式|微批次|流式|流式
容错开销|高|低|取决于分布式事务存储的吞吐量|低
流控制|有问题|有问题|自然|自然
应用程序逻辑与容错分离|部分（超时很重要）|否（微批量大小会影响语义）|是|是














原文：https://data-artisans.com/blog/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink
